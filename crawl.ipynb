{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333db87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE = \"https://dantri.com.vn\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0 Safari/537.36\"\n",
    "}\n",
    "TIMEOUT = 25\n",
    "\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"Xã hội\": \"/xa-hoi.htm\",\n",
    "    \"Kinh doanh\": \"/kinh-doanh.htm\",\n",
    "    \"Đời sống\": \"/doi-song.htm\",\n",
    "    \"Sức khỏe\": \"/suc-khoe.htm\",\n",
    "    \"Pháp luật\": \"/phap-luat.htm\",\n",
    "    \"Thế giới\": \"/the-gioi.htm\",\n",
    "    \"Khoa học\": \"/khoa-hoc.htm\",\n",
    "    \"Thể thao\": \"/the-thao.htm\",\n",
    "    \"Giải trí\": \"/giai-tri.htm\",\n",
    "    \"Du lịch\": \"/du-lich.htm\",\n",
    "    \"Giáo dục\": \"/giao-duc.htm\"\n",
    "}\n",
    "\n",
    "\n",
    "MAX_PAGES_PER_CAT = 10\n",
    "\n",
    "ARTICLE_LINK_SELECTORS = [\n",
    "    \"h3 a[href*='.htm']\",\n",
    "    \"article a[href*='.htm']\",\n",
    "    \"a.article-title[href*='.htm']\",\n",
    "    \"a.dt-news__title[href*='.htm']\"\n",
    "]\n",
    "CONTENT_CONTAINERS = [\n",
    "    \"article\",\n",
    "    \"div#dantri-detail-content\",\n",
    "    \"div.dt-detail__content\",\n",
    "    \"div.detail__content\",\n",
    "    \"div.singular-content\",\n",
    "    \"div.article__content\",\n",
    "    \"div[itemprop='articleBody']\",\n",
    "]\n",
    "IMG_ATTRS = [\"data-src\", \"data-original\", \"data-echo\", \"src\", \"data-srcset\", \"srcset\"]\n",
    "VALID_IMG_EXT = (\".jpg\", \".jpeg\", \".png\", \".webp\", \".gif\")\n",
    "EXCLUDE_IN_PATH = {\"video\", \"clip\", \"photo\", \"infographic\", \"podcast\", \"tag\"}\n",
    "PAGE_TAIL_RE = re.compile(r\"(-trang-\\d+\\.htm$|-p\\d+\\.htm$)\", re.I)\n",
    "KILL_PREFIXES = [\n",
    "    \"ảnh:\", \"video:\", \"xem thêm\", \"mời độc giả\", \"độc giả\", \"bình luận\",\n",
    "    \"theo:\", \"nguồn:\", \"liên hệ quảng cáo\", \"xem thêm về:\"\n",
    "]\n",
    "KILL_REGEX = re.compile(r\"^\\s*(%s)\" % \"|\".join([re.escape(k) for k in KILL_PREFIXES]), re.I)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def get_soup(url, retries=3, backoff=0.6):\n",
    "    last_err = None\n",
    "    for k in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            r.encoding = \"utf-8\"\n",
    "            r.raise_for_status()\n",
    "            return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"  ! Retry {k+1}/{retries} failed for {url}: {e}\")\n",
    "            time.sleep(backoff * (k + 1))\n",
    "    raise last_err\n",
    "\n",
    "def clean_url(href, base=BASE):\n",
    "    if not href: return None\n",
    "    href = href.strip()\n",
    "    if href.startswith(\"#\"): return None\n",
    "    return urljoin(base, href)\n",
    "\n",
    "def is_same_domain(url: str, domain: str = \"dantri.com.vn\"):\n",
    "    try:\n",
    "        return urlparse(url).netloc.endswith(domain)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def path_parts(url: str):\n",
    "    return [x for x in urlparse(url).path.split(\"/\") if x]\n",
    "\n",
    "def is_article_url(href: str) -> bool:\n",
    "    if not href:\n",
    "        return False\n",
    "    if not href.endswith(\".htm\"):\n",
    "        return False\n",
    "    parts = path_parts(href)\n",
    "    if any(x in parts for x in EXCLUDE_IN_PATH):\n",
    "        return False\n",
    "    if PAGE_TAIL_RE.search(urlparse(href).path):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def find_next_page_url(soup):\n",
    "    ln = soup.find(\"link\", rel=lambda x: x and \"next\" in x.lower())\n",
    "    if ln and ln.get(\"href\"): return clean_url(ln[\"href\"])\n",
    "    a = soup.select_one(\n",
    "        \"a[rel='next'], a.next, a[aria-label*='Next' i], a[aria-label*='Sau' i], \"\n",
    "        \"li.pagination__next a[href], a[title*='Sau' i]\"\n",
    "    )\n",
    "    if a and a.get(\"href\"): return clean_url(a[\"href\"])\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        txt = (a.get_text() or \"\").strip().lower()\n",
    "        if any(k in txt for k in [\"sau\", \"next\", \"trang sau\", \"»\", \">\"]):\n",
    "            if a.get(\"href\"): return clean_url(a[\"href\"])\n",
    "    return None\n",
    "\n",
    "def _parse_srcset(val: str):\n",
    "    if not val: return []\n",
    "    return [p.strip().split(\" \")[0] for p in val.split(\",\") if p.strip()]\n",
    "\n",
    "def _pick_best_from_srcset(val: str):\n",
    "    if not val: return None\n",
    "    best_url, best_w = None, -1\n",
    "    for part in val.split(\",\"):\n",
    "        toks = part.strip().split()\n",
    "        if not toks: continue\n",
    "        url = toks[0]; width = -1\n",
    "        for t in toks[1:]:\n",
    "            m = re.match(r\"(\\d+)(w|x)$\", t)\n",
    "            if m: width = int(m.group(1)); break\n",
    "        if width > best_w: best_w = width; best_url = url\n",
    "    return clean_url(best_url)\n",
    "\n",
    "# ---------- listing ----------\n",
    "def parse_listing_articles(cat_url, delay=0.25, max_pages=None):\n",
    "    out, visited = [], set()\n",
    "    url, page_count = clean_url(cat_url), 0\n",
    "\n",
    "    while url and url not in visited:\n",
    "        visited.add(url)\n",
    "        soup = get_soup(url)\n",
    "\n",
    "        found, page_links = 0, []\n",
    "        for sel in ARTICLE_LINK_SELECTORS:\n",
    "            for a in soup.select(sel):\n",
    "                href = clean_url(a.get(\"href\"))\n",
    "                if not href or not is_same_domain(href) or not is_article_url(href):\n",
    "                    continue\n",
    "                if href in page_links:\n",
    "                    continue\n",
    "                page_links.append(href)\n",
    "                out.append({\"title\": a.get_text(strip=True) or \"\", \"link\": href})\n",
    "                found += 1\n",
    "\n",
    "        print(f\"    -> Found {found} article links at:\", url)\n",
    "\n",
    "        next_url = find_next_page_url(soup)\n",
    "        if not next_url:\n",
    "            break\n",
    "        url = next_url\n",
    "        page_count += 1\n",
    "        if max_pages is not None and page_count >= max_pages:\n",
    "            break\n",
    "        time.sleep(delay)\n",
    "\n",
    "    uniq = {}\n",
    "    for r in out:\n",
    "        if r[\"link\"] not in uniq:\n",
    "            uniq[r[\"link\"]] = r\n",
    "    return list(uniq.values())\n",
    "\n",
    "\n",
    "def extract_article(article_url):\n",
    "    soup = get_soup(article_url)\n",
    "\n",
    "    # Title\n",
    "    title = \"\"\n",
    "    h1 = soup.select_one(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        title = h1.get_text(strip=True)\n",
    "    if not title:\n",
    "        og = soup.select_one(\"meta[property='og:title']\")\n",
    "        if og and og.get(\"content\"):\n",
    "            title = og[\"content\"].strip()\n",
    "    if not title:\n",
    "        tt = soup.find(\"title\")\n",
    "        if tt:\n",
    "            title = tt.get_text(strip=True)\n",
    "\n",
    "    # Content container\n",
    "    CONTENT_CONTAINERS = [\n",
    "        \"article\", \"div#dantri-detail-content\", \"div.dt-detail__content\", \"div.detail__content\",\n",
    "        \"div.singular-content\", \"div.article__content\", \"div[itemprop='articleBody']\"\n",
    "    ]\n",
    "    container = None\n",
    "    for sel in CONTENT_CONTAINERS:\n",
    "        container = soup.select_one(sel)\n",
    "        if container and container.find([\"p\", \"li\", \"img\"]):\n",
    "            break\n",
    "    if not container:\n",
    "        container = soup.body or soup\n",
    "\n",
    "    # Content text\n",
    "    paras = []\n",
    "    for tag in container.find_all([\"p\", \"li\"]):\n",
    "        t = tag.get_text(\" \", strip=True)\n",
    "        if not t:\n",
    "            continue\n",
    "        if KILL_REGEX.search(t):\n",
    "            continue\n",
    "        if re.match(r\"^\\s*xem thêm\\s*:?\", t, flags=re.I):\n",
    "            continue\n",
    "        if len(t) < 20:\n",
    "            continue\n",
    "        paras.append(t)\n",
    "    content = \"\\n\".join(paras).strip()\n",
    "    if not content:\n",
    "        return None\n",
    "\n",
    "    # Hero image\n",
    "    hero = \"\"\n",
    "    for meta_sel in [\"meta[property='og:image']\", \"meta[name='twitter:image']\"]:\n",
    "        ogimg = soup.select_one(meta_sel)\n",
    "        if ogimg and ogimg.get(\"content\"):\n",
    "            u = clean_url(ogimg[\"content\"])\n",
    "            if u:\n",
    "                hero = u\n",
    "                break\n",
    "    if not hero:\n",
    "        first_img = container.find(\"img\")\n",
    "        if first_img:\n",
    "            if first_img.get(\"srcset\") or first_img.get(\"data-srcset\"):\n",
    "                best = _pick_best_from_srcset(first_img.get(\"srcset\") or first_img.get(\"data-srcset\"))\n",
    "                if best:\n",
    "                    hero = best\n",
    "            if not hero:\n",
    "                for attr in IMG_ATTRS:\n",
    "                    val = first_img.get(attr)\n",
    "                    if val:\n",
    "                        if \"srcset\" in attr:\n",
    "                            for cand in _parse_srcset(val):\n",
    "                                cu = clean_url(cand)\n",
    "                                if cu:\n",
    "                                    hero = cu\n",
    "                                    break\n",
    "                            if hero:\n",
    "                                break\n",
    "                        else:\n",
    "                            cu = clean_url(val)\n",
    "                            if cu:\n",
    "                                hero = cu\n",
    "                                break\n",
    "\n",
    "\n",
    "    published_date, author = \"\", \"\"\n",
    "    for sel in [\n",
    "        'meta[property=\"article:published_time\"]',\n",
    "        'meta[name=\"pubdate\"]',\n",
    "        'meta[itemprop=\"datePublished\"]',\n",
    "        'time[datetime]'\n",
    "    ]:\n",
    "        tag = soup.select_one(sel)\n",
    "        if tag:\n",
    "            published_date = (tag.get(\"content\") or tag.get(\"datetime\") or \"\").strip()\n",
    "            if published_date:\n",
    "                break\n",
    "    auth_tag = soup.select_one('meta[name=\"author\"], a[rel=\"author\"], .author, .dt-news__author, .article__author')\n",
    "    if auth_tag:\n",
    "        author = (auth_tag.get(\"content\") or auth_tag.get_text(\" \", strip=True) or \"\").strip()\n",
    "\n",
    "    return title.strip(), content.strip(), (hero or \"\"), (published_date or \"\"), (author or \"\")\n",
    "\n",
    "\n",
    "def run_crawl(per_article_delay=0.15,\n",
    "              max_pages_root=MAX_PAGES_PER_CAT,\n",
    "              out_json=\"dantri_articles_root.json\"):\n",
    "    records, seen_urls = [], set()\n",
    "    print(\"CATEGORIES:\", {k: urljoin(BASE, v) for k, v in CATEGORIES.items()})\n",
    "\n",
    "    for cat_name, cat_path in CATEGORIES.items():\n",
    "        cat_url = urljoin(BASE, cat_path)\n",
    "        print(f\"\\n=== {cat_name} => {cat_url}\")\n",
    "        links = parse_listing_articles(cat_url, delay=0.25, max_pages=max_pages_root)\n",
    "        print(f\"  + root listing: {len(links)} links (no cap)\")\n",
    "\n",
    "        for i, item in enumerate(links, 1):\n",
    "            if item[\"link\"] in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(item[\"link\"])\n",
    "\n",
    "            try:\n",
    "                parsed = extract_article(item[\"link\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  ! Parse error: {item['link']} -> {e}\")\n",
    "                continue\n",
    "            if not parsed:\n",
    "                continue\n",
    "            title, content, hero, pub, author = parsed\n",
    "\n",
    "            rec = {\n",
    "                \"url\": item[\"link\"],\n",
    "                \"image-url\": hero,\n",
    "                \"title\": title or item[\"title\"],\n",
    "                \"content\": content,\n",
    "                \"metadata\": {\n",
    "                    \"cat\": cat_name,\n",
    "                    \"published_date\": pub,\n",
    "                    \"author\": author\n",
    "                }\n",
    "            }\n",
    "            records.append(rec)\n",
    "\n",
    "            # progress mỗi 10 bài\n",
    "            if i % 10 == 0 or i == len(links):\n",
    "                print(f\"    [{i}/{len(links)}] root {cat_name}\")\n",
    "\n",
    "            time.sleep(per_article_delay)\n",
    "\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"\\nSaved JSON: {out_json} (total records: {len(records)})\")\n",
    "    return records\n",
    "\n",
    "# ---- RUN ----\n",
    "if __name__ == \"__main__\":\n",
    "    run_crawl(\n",
    "        per_article_delay=0.15,\n",
    "        max_pages_root=MAX_PAGES_PER_CAT,\n",
    "        out_json=\"dantri_articles_root.json\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
